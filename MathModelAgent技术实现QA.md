# MathModelAgent æŠ€æœ¯å®ç° Q&A

## ğŸ—ï¸ æ¡†æ¶å’Œæ¶æ„ç›¸å…³

### Q: MathModelAgent ä½¿ç”¨äº†ä»€ä¹ˆæ¡†æ¶ï¼Ÿ
**A**: é¡¹ç›®é‡‡ç”¨**æ— æ¡†æ¶(Agentless)**è®¾è®¡ç†å¿µï¼Œæ²¡æœ‰ä½¿ç”¨ç°æˆçš„ Agent æ¡†æ¶å¦‚ LangChainã€AutoGen ç­‰ã€‚

**æŠ€æœ¯æ ˆ**ï¼š
- **åç«¯**: FastAPI (Python å¼‚æ­¥ Web æ¡†æ¶)
- **å‰ç«¯**: Vue 3 + TypeScript + Vite
- **LLMé›†æˆ**: LiteLLM (æ”¯æŒ100+æ¨¡å‹æä¾›å•†)
- **ä»£ç æ‰§è¡Œ**: Jupyter Kernel (æœ¬åœ°) + E2B (äº‘ç«¯)
- **æ¶ˆæ¯é˜Ÿåˆ—**: Redis
- **å®æ—¶é€šä¿¡**: WebSocket

### Q: ä¸ºä»€ä¹ˆé€‰æ‹©æ— æ¡†æ¶è®¾è®¡ï¼Ÿ
**A**: 
1. **æˆæœ¬æ§åˆ¶**: é¿å…æ¡†æ¶çš„é¢å¤–å¼€é”€å’Œå¤æ‚æ€§
2. **çµæ´»æ€§**: å¯ä»¥å®Œå…¨æŒ‰éœ€å®šåˆ¶ï¼Œä¸å—æ¡†æ¶é™åˆ¶
3. **è½»é‡åŒ–**: å‡å°‘ä¾èµ–ï¼Œæé«˜æ€§èƒ½
4. **ä¸“ä¸šåŒ–**: é’ˆå¯¹æ•°å­¦å»ºæ¨¡åœºæ™¯æ·±åº¦ä¼˜åŒ–

## ğŸ¤– Agent å®ç°ç›¸å…³

### Q: Agent æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ
**A**: é‡‡ç”¨**ç»§æ‰¿-ç»„åˆ**æ¨¡å¼å®ç°ï¼š

```python
# 1. åŸºç¡€ Agent ç±»
class Agent:
    def __init__(self, task_id: str, model: LLM, max_chat_turns: int = 30):
        self.task_id = task_id
        self.model = model
        self.chat_history: list[dict] = []  # å¯¹è¯å†å²
        self.max_chat_turns = max_chat_turns
        
    async def run(self, prompt: str, system_prompt: str) -> str:
        # ç»Ÿä¸€çš„å¯¹è¯æ‰§è¡Œé€»è¾‘
        pass
        
    async def clear_memory(self):
        # æ™ºèƒ½å†…å­˜ç®¡ç†
        pass

# 2. ä¸“ä¸šåŒ– Agent
class CoderAgent(Agent):
    def __init__(self, task_id, model, code_interpreter):
        super().__init__(task_id, model)
        self.code_interpreter = code_interpreter
        
    async def run(self, prompt: str) -> CoderToWriter:
        # ä¸“é—¨çš„ä»£ç æ‰§è¡Œé€»è¾‘
        while True:
            response = await self.model.chat(tools=coder_tools)
            if has_tool_calls:
                # æ‰§è¡Œä»£ç 
                result = await self.code_interpreter.execute_code(code)
            else:
                # ä»»åŠ¡å®Œæˆ
                return result
```

**æ ¸å¿ƒç‰¹ç‚¹**ï¼š
- **ç»Ÿä¸€åŸºç±»**: æä¾›å¯¹è¯ç®¡ç†ã€å†…å­˜æ¸…ç†ç­‰é€šç”¨åŠŸèƒ½
- **ä¸“ä¸šåŒ–ç»§æ‰¿**: æ¯ä¸ªAgentä¸“æ³¨ç‰¹å®šé¢†åŸŸ
- **å·¥å…·é›†æˆ**: é€šè¿‡å·¥å…·è°ƒç”¨æ‰©å±•èƒ½åŠ›

### Q: å››ä¸ª Agent åˆ†åˆ«è´Ÿè´£ä»€ä¹ˆï¼Ÿ
**A**:

| Agent | èŒè´£ | è¾“å…¥ | è¾“å‡º | ç‰¹æ®Šèƒ½åŠ› |
|-------|------|------|------|----------|
| **CoordinatorAgent** | é—®é¢˜ç†è§£å’Œæ ¼å¼åŒ– | ç”¨æˆ·åŸå§‹é—®é¢˜ | ç»“æ„åŒ–JSON | æ™ºèƒ½åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å­¦å»ºæ¨¡é—®é¢˜ |
| **ModelerAgent** | æ•°å­¦å»ºæ¨¡è®¾è®¡ | æ ¼å¼åŒ–é—®é¢˜ | å»ºæ¨¡æ–¹æ¡ˆJSON | ä¸“ä¸šæ•°å­¦å»ºæ¨¡çŸ¥è¯† |
| **CoderAgent** | ä»£ç å®ç°æ‰§è¡Œ | å»ºæ¨¡æ–¹æ¡ˆ | ä»£ç +å›¾ç‰‡ | ä»£ç æ‰§è¡Œã€é”™è¯¯ä¿®æ­£ã€é‡è¯• |
| **WriterAgent** | å­¦æœ¯è®ºæ–‡æ’°å†™ | ä»£ç ç»“æœ | è®ºæ–‡ç« èŠ‚ | æ–‡çŒ®æœç´¢ã€å›¾ç‰‡å¼•ç”¨ |

### Q: Agent ä¹‹é—´æ€ä¹ˆåä½œçš„ï¼Ÿ
**A**: é‡‡ç”¨**çº¿æ€§æµæ°´çº¿**æ¨¡å¼ï¼š

```python
# å·¥ä½œæµç¼–æ’
class MathModelWorkFlow:
    async def execute(self, problem: Problem):
        # 1. åè°ƒå‘˜å¤„ç†é—®é¢˜
        coordinator_response = await coordinator_agent.run(problem.ques_all)
        
        # 2. å»ºæ¨¡æ‰‹è®¾è®¡æ–¹æ¡ˆ  
        modeler_response = await modeler_agent.run(coordinator_response)
        
        # 3. ä»£ç æ‰‹å®ç°æ±‚è§£
        for key, value in solution_flows.items():
            coder_response = await coder_agent.run(value["coder_prompt"])
            writer_response = await writer_agent.run(writer_prompt, coder_response.images)
            
        # 4. å†™ä½œæ‰‹å®Œæˆè®ºæ–‡
        for key, value in write_flows.items():
            writer_response = await writer_agent.run(value)
```

**åä½œç‰¹ç‚¹**ï¼š
- **é¡ºåºæ‰§è¡Œ**: Agent æŒ‰å›ºå®šé¡ºåºæ‰§è¡Œï¼Œåç»­ Agent ä¾èµ–å‰é¢çš„ç»“æœ
- **æ•°æ®ä¼ é€’**: é€šè¿‡æ ‡å‡†åŒ–çš„æ•°æ®ç»“æ„ä¼ é€’ä¿¡æ¯
- **æµç¨‹æ§åˆ¶**: é€šè¿‡ Flows ç±»ç»Ÿä¸€ç®¡ç†æ‰§è¡Œæµç¨‹

## ğŸ’» ä»£ç æ‰§è¡Œç›¸å…³

### Q: ä»£ç æ‰§è¡Œç¯å¢ƒæ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ
**A**: æ”¯æŒ**åŒæ¨¡å¼**ä»£ç æ‰§è¡Œï¼š

**1. æœ¬åœ°æ¨¡å¼ (LocalCodeInterpreter)**
```python
class LocalCodeInterpreter(BaseCodeInterpreter):
    def __init__(self, work_dir: str):
        self.kernel_manager = AsyncKernelManager()
        self.work_dir = work_dir
        
    async def execute_code(self, code: str):
        # 1. å¯åŠ¨ Jupyter Kernel
        await self.kernel_manager.start_kernel()
        
        # 2. æ‰§è¡Œä»£ç 
        reply = await self.kernel_client.execute(code)
        
        # 3. å¤„ç†ç»“æœå’Œé”™è¯¯
        return self._process_reply(reply)
```

**2. äº‘ç«¯æ¨¡å¼ (E2BCodeInterpreter)**
```python
class E2BCodeInterpreter(BaseCodeInterpreter):
    @classmethod
    async def create(cls, task_id: str):
        # åˆ›å»º E2B æ²™ç›’
        sandbox = await Sandbox.create(template="base")
        return cls(sandbox, task_id)
        
    async def execute_code(self, code: str):
        # åœ¨äº‘ç«¯æ²™ç›’ä¸­æ‰§è¡Œ
        result = await self.sandbox.run_code(code)
        return self._process_result(result)
```

**æ™ºèƒ½é€‰æ‹©ç­–ç•¥**ï¼š
```python
async def create_interpreter(kind: Literal["remote", "local"] = "local"):
    if not settings.E2B_API_KEY:
        logger.info("é»˜è®¤ä½¿ç”¨æœ¬åœ°è§£é‡Šå™¨")
        kind = "local"
    else:
        logger.info("ä½¿ç”¨è¿œç¨‹è§£é‡Šå™¨") 
        kind = "remote"
```

### Q: ä»£ç æ‰§è¡Œçš„é”™è¯¯å¤„ç†æ€ä¹ˆåšï¼Ÿ
**A**: å®ç°äº†**æ™ºèƒ½é‡è¯•**æœºåˆ¶ï¼š

```python
class CoderAgent(Agent):
    async def run(self, prompt: str) -> CoderToWriter:
        retry_count = 0
        while retry_count < self.max_retries:
            try:
                # æ‰§è¡Œä»£ç 
                result = await self.code_interpreter.execute_code(code)
                if error_occurred:
                    # ç”Ÿæˆåæ€æç¤º
                    reflection_prompt = get_reflection_prompt(error_message, code)
                    await self.append_chat_history({
                        "role": "user", 
                        "content": reflection_prompt
                    })
                    retry_count += 1
                    continue
                else:
                    return result
            except Exception as e:
                retry_count += 1
                continue
```

**é”™è¯¯å¤„ç†ç‰¹ç‚¹**ï¼š
- **è‡ªåŠ¨æ£€æµ‹**: è‡ªåŠ¨è¯†åˆ«ä»£ç æ‰§è¡Œé”™è¯¯
- **æ™ºèƒ½åæ€**: ä½¿ç”¨ LLM åˆ†æé”™è¯¯åŸå› å¹¶ç”Ÿæˆä¿®æ­£æç¤º
- **é‡è¯•é™åˆ¶**: é˜²æ­¢æ— é™å¾ªç¯
- **é”™è¯¯è®°å½•**: ä¿å­˜é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•

## ğŸ”„ å·¥ä½œæµç¼–æ’ç›¸å…³

### Q: å·¥ä½œæµæ˜¯æ€ä¹ˆè®¾è®¡çš„ï¼Ÿ
**A**: é‡‡ç”¨**åŒé˜¶æ®µæµç¨‹**è®¾è®¡ï¼š

**é˜¶æ®µ1: è§£å†³æ–¹æ¡ˆæµç¨‹ (Solution Flows)**
```python
solution_flows = {
    "eda": {
        "coder_prompt": "å¯¹æ•°æ®è¿›è¡ŒEDAåˆ†æå’Œå¯è§†åŒ–"
    },
    "ques1": {
        "coder_prompt": f"å‚è€ƒå»ºæ¨¡æ–¹æ¡ˆ{modeler_response.ques1}å®Œæˆé—®é¢˜1"
    },
    "ques2": {
        "coder_prompt": f"å‚è€ƒå»ºæ¨¡æ–¹æ¡ˆ{modeler_response.ques2}å®Œæˆé—®é¢˜2"  
    },
    "sensitivity_analysis": {
        "coder_prompt": "å®Œæˆæ•æ„Ÿæ€§åˆ†æ"
    }
}
```

**é˜¶æ®µ2: å†™ä½œæµç¨‹ (Write Flows)**
```python
write_flows = {
    "firstPage": "æ’°å†™å°é¢ã€æ‘˜è¦ã€å…³é”®è¯",
    "RepeatQues": "æ’°å†™é—®é¢˜é‡è¿°", 
    "analysisQues": "æ’°å†™é—®é¢˜åˆ†æ",
    "modelAssumption": "æ’°å†™æ¨¡å‹å‡è®¾",
    "symbol": "æ’°å†™ç¬¦å·è¯´æ˜",
    "judge": "æ’°å†™æ¨¡å‹è¯„ä»·"
}
```

### Q: æµç¨‹æ˜¯æ€ä¹ˆç®¡ç†çš„ï¼Ÿ
**A**: é€šè¿‡ **Flows ç±»**ç»Ÿä¸€ç®¡ç†ï¼š

```python
class Flows:
    def __init__(self, questions: dict):
        self.questions = questions
        
    def get_solution_flows(self, modeler_response):
        # åŠ¨æ€ç”Ÿæˆæ±‚è§£æµç¨‹
        return solution_flows
        
    def get_write_flows(self, user_output, config_template):
        # åŠ¨æ€ç”Ÿæˆå†™ä½œæµç¨‹  
        return write_flows
        
    def get_writer_prompt(self, key: str, coder_response: str):
        # ç”Ÿæˆç‰¹å®šçš„å†™ä½œæç¤º
        return writer_prompt
```

## ğŸŒ å‰åç«¯é€šä¿¡ç›¸å…³

### Q: å®æ—¶é€šä¿¡æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ
**A**: é‡‡ç”¨ **WebSocket + Redis** æ¶æ„ï¼š

**åç«¯å‘å¸ƒæ¶ˆæ¯**ï¼š
```python
# åœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­å‘å¸ƒçŠ¶æ€æ›´æ–°
await redis_manager.publish_message(
    task_id,
    SystemMessage(content="ä»£ç æ‰‹å¼€å§‹æ±‚è§£é—®é¢˜1", type="info")
)

await redis_manager.publish_message(
    task_id, 
    InterpreterMessage(input={"code": code})
)
```

**å‰ç«¯è®¢é˜…æ¶ˆæ¯**ï¼š
```typescript
// WebSocket è¿æ¥
const ws = new WebSocket(`ws://localhost:8000/ws/${taskId}`)

ws.onmessage = (event) => {
    const message = JSON.parse(event.data)
    switch(message.type) {
        case 'system':
            updateSystemStatus(message.content)
            break
        case 'interpreter':
            displayCodeExecution(message.input)
            break
        case 'writer':
            showWritingProgress(message.content)
            break
    }
}
```

**æ¶ˆæ¯ç±»å‹è®¾è®¡**ï¼š
```python
class SystemMessage(BaseModel):
    content: str
    type: Literal["info", "error", "success"] = "info"

class InterpreterMessage(BaseModel):
    input: dict  # ä»£ç æ‰§è¡Œè¾“å…¥

class WriterMessage(BaseModel):
    input: dict  # å†™ä½œè¾“å…¥
```

### Q: å‰ç«¯æ¶æ„æ˜¯æ€æ ·çš„ï¼Ÿ
**A**: é‡‡ç”¨ **Vue 3 + TypeScript** ç°ä»£åŒ–æ¶æ„ï¼š

**çŠ¶æ€ç®¡ç† (Pinia)**ï¼š
```typescript
// stores/task.ts
export const useTaskStore = defineStore('task', {
    state: () => ({
        currentTask: null,
        messages: [],
        status: 'idle'
    }),
    actions: {
        async submitTask(problem: Problem) {
            const response = await submitModelingApi(problem)
            this.currentTask = response.data
        }
    }
})
```

**ç»„ä»¶è®¾è®¡**ï¼š
```vue
<!-- ChatArea.vue -->
<template>
    <div class="chat-container">
        <div v-for="message in messages" :key="message.id">
            <SystemMessage v-if="message.type === 'system'" :message="message" />
            <InterpreterMessage v-if="message.type === 'interpreter'" :message="message" />
        </div>
    </div>
</template>
```

## ğŸ”§ LLM é›†æˆç›¸å…³

### Q: æ€ä¹ˆé›†æˆå¤šç§ LLM æ¨¡å‹ï¼Ÿ
**A**: é€šè¿‡ **LiteLLM + å·¥å‚æ¨¡å¼**ï¼š

**LLM å°è£…**ï¼š
```python
class LLM:
    def __init__(self, api_key: str, model: str, base_url: str):
        self.api_key = api_key
        self.model = model  
        self.base_url = base_url
        
    async def chat(self, history: list, tools: list = None):
        # ä½¿ç”¨ LiteLLM ç»Ÿä¸€æ¥å£
        response = await litellm.achat(
            model=self.model,
            messages=history,
            tools=tools,
            api_key=self.api_key,
            base_url=self.base_url
        )
        return response
```

**å·¥å‚æ¨¡å¼åˆ†é…**ï¼š
```python
class LLMFactory:
    def get_all_llms(self) -> tuple[LLM, LLM, LLM, LLM]:
        # æ¯ä¸ª Agent ä½¿ç”¨ä¸åŒçš„æ¨¡å‹é…ç½®
        coordinator_llm = LLM(
            model=settings.COORDINATOR_MODEL,  # å¦‚: gpt-4o-mini
            api_key=settings.COORDINATOR_API_KEY
        )
        
        coder_llm = LLM(
            model=settings.CODER_MODEL,  # å¦‚: claude-3.5-sonnet  
            api_key=settings.CODER_API_KEY
        )
        
        return coordinator_llm, modeler_llm, coder_llm, writer_llm
```

### Q: å·¥å…·è°ƒç”¨æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ
**A**: åŸºäº **OpenAI Function Calling** æ ‡å‡†ï¼š

**å·¥å…·å®šä¹‰**ï¼š
```python
coder_tools = [
    {
        "type": "function",
        "function": {
            "name": "execute_code",
            "description": "æ‰§è¡ŒPythonä»£ç ",
            "parameters": {
                "type": "object", 
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "è¦æ‰§è¡Œçš„Pythonä»£ç "
                    }
                },
                "required": ["code"]
            }
        }
    }
]

writer_tools = [
    {
        "type": "function",
        "function": {
            "name": "search_papers", 
            "description": "æœç´¢å­¦æœ¯è®ºæ–‡",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯"
                    }
                },
                "required": ["query"]
            }
        }
    }
]
```

**å·¥å…·æ‰§è¡Œ**ï¼š
```python
if response.choices[0].message.tool_calls:
    tool_call = response.choices[0].message.tool_calls[0]
    
    if tool_call.function.name == "execute_code":
        args = json.loads(tool_call.function.arguments)
        result = await self.code_interpreter.execute_code(args["code"])
        
        # æ·»åŠ å·¥å…·æ‰§è¡Œç»“æœåˆ°å¯¹è¯å†å²
        self.chat_history.append({
            "role": "tool",
            "tool_call_id": tool_call.id,
            "name": "execute_code", 
            "content": result
        })
```

## ğŸ› ï¸ å…³é”®æŠ€æœ¯ç»†èŠ‚

### Q: å†…å­˜ç®¡ç†æ˜¯æ€ä¹ˆåšçš„ï¼Ÿ
**A**: å®ç°äº†**æ™ºèƒ½å†…å­˜å‹ç¼©**æœºåˆ¶ï¼š

```python
class Agent:
    async def clear_memory(self):
        if len(self.chat_history) <= self.max_memory:
            return
            
        # 1. ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
        system_msg = self.chat_history[0] if self.chat_history[0]["role"] == "system" else None
        
        # 2. æ‰¾åˆ°å®‰å…¨çš„ä¿ç•™ç‚¹(ä¸ç ´åå·¥å…·è°ƒç”¨)
        preserve_start_idx = self._find_safe_preserve_point()
        
        # 3. æ€»ç»“éœ€è¦å‹ç¼©çš„å†å²
        summary_history = self.chat_history[1:preserve_start_idx]
        summary = await simple_chat(self.model, [{
            "role": "user",
            "content": f"è¯·ç®€æ´æ€»ç»“ä»¥ä¸‹å¯¹è¯å†…å®¹ï¼š{summary_history}"
        }])
        
        # 4. é‡æ„å†å²ï¼šç³»ç»Ÿæ¶ˆæ¯ + æ€»ç»“ + ä¿ç•™æ¶ˆæ¯
        new_history = [system_msg] if system_msg else []
        new_history.append({"role": "assistant", "content": f"[å†å²æ€»ç»“] {summary}"})
        new_history.extend(self.chat_history[preserve_start_idx:])
        
        self.chat_history = new_history
```

### Q: æ¨¡æ¿åŒ–æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ
**A**: é€šè¿‡ **TOML é…ç½®æ–‡ä»¶**ï¼š

**é…ç½®æ–‡ä»¶** (`md_template.toml`):
```toml
[template]
firstPage = """
# {title}

## æ‘˜è¦
{abstract}

## å…³é”®è¯  
{keywords}
"""

eda = """
## æ•°æ®åˆ†æ

### æ•°æ®æ¦‚è¿°
{data_overview}

### å¯è§†åŒ–åˆ†æ
{visualization}
"""
```

**åŠ¨æ€åŠ è½½**ï¼š
```python
def get_config_template(comp_template: CompTemplate) -> dict:
    with open("config/md_template.toml", "rb") as f:
        config = tomli.load(f)
    return config["template"]

# ä½¿ç”¨æ¨¡æ¿
writer_prompt = f"""
æ ¹æ®ä»¥ä¸‹æ¨¡æ¿æ’°å†™ï¼š{config_template["eda"]}
ä»£ç æ‰§è¡Œç»“æœï¼š{coder_response}
"""
```

## ğŸš€ éƒ¨ç½²å’Œè¿ç»´

### Q: é¡¹ç›®æ€ä¹ˆéƒ¨ç½²ï¼Ÿ
**A**: æ”¯æŒ**ä¸‰ç§éƒ¨ç½²æ–¹å¼**ï¼š

**1. Docker éƒ¨ç½² (æ¨è)**ï¼š
```yaml
# docker-compose.yml
version: '3.8'
services:
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
      
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    depends_on:
      - redis
      
  frontend:
    build: ./frontend  
    ports:
      - "5173:5173"
    depends_on:
      - backend
```

**2. æœ¬åœ°éƒ¨ç½²**ï¼š
```bash
# åç«¯
cd backend
uv sync
source .venv/bin/activate
ENV=DEV uvicorn app.main:app --reload

# å‰ç«¯  
cd frontend
pnpm install
pnpm dev
```

**3. è‡ªåŠ¨åŒ–è„šæœ¬**ï¼š
- ç¤¾åŒºæä¾›çš„ä¸€é”®éƒ¨ç½²è„šæœ¬
- è‡ªåŠ¨é…ç½®ç¯å¢ƒå’Œä¾èµ–

### Q: é…ç½®ç®¡ç†æ€ä¹ˆåšï¼Ÿ
**A**: ä½¿ç”¨ **Pydantic Settings**ï¼š

```python
class Settings(BaseSettings):
    # LLM é…ç½®
    COORDINATOR_MODEL: str = "gpt-4o-mini"
    COORDINATOR_API_KEY: str
    COORDINATOR_BASE_URL: str = "https://api.openai.com/v1"
    
    CODER_MODEL: str = "claude-3.5-sonnet"
    CODER_API_KEY: str
    
    # æ‰§è¡Œé…ç½®
    MAX_CHAT_TURNS: int = 30
    MAX_RETRIES: int = 3
    
    # Redis é…ç½®
    REDIS_URL: str = "redis://localhost:6379"
    
    class Config:
        env_file = ".env.dev"

settings = Settings()
```

## ğŸ’¡ è®¾è®¡æ€æƒ³æ€»ç»“

### Q: è¿™ä¸ªé¡¹ç›®çš„æ ¸å¿ƒè®¾è®¡ç†å¿µæ˜¯ä»€ä¹ˆï¼Ÿ
**A**: 

**1. ä¸“ä¸šåŒ–åˆ†å·¥**ï¼š
- æ¯ä¸ª Agent ä¸“æ³¨ç‰¹å®šé¢†åŸŸï¼Œé¿å…"ä¸‡èƒ½Agent"çš„å¤æ‚æ€§
- æ¸…æ™°çš„èŒè´£è¾¹ç•Œï¼Œä¾¿äºç»´æŠ¤å’Œä¼˜åŒ–

**2. æ— æ¡†æ¶è½»é‡åŒ–**ï¼š
- é¿å…é‡å‹æ¡†æ¶çš„æŸç¼šå’Œå¼€é”€
- é’ˆå¯¹æ•°å­¦å»ºæ¨¡åœºæ™¯æ·±åº¦å®šåˆ¶

**3. å·¥ç¨‹åŒ–å®è·µ**ï¼š
- å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- å®æ—¶è¿›åº¦åé¦ˆå’ŒçŠ¶æ€ç®¡ç†
- æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•

**4. ç”¨æˆ·ä½“éªŒä¼˜å…ˆ**ï¼š
- ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–ï¼Œé™ä½ä½¿ç”¨é—¨æ§›
- å®æ—¶åé¦ˆï¼Œæå‡äº¤äº’ä½“éªŒ
- æ ‡å‡†åŒ–è¾“å‡ºï¼Œç¬¦åˆæ¯”èµ›è¦æ±‚

è¿™ç§è®¾è®¡ä½¿å¾— MathModelAgent åœ¨æ•°å­¦å»ºæ¨¡è‡ªåŠ¨åŒ–é¢†åŸŸå®ç°äº†å¾ˆå¥½çš„å¹³è¡¡ï¼šæ—¢ä¿æŒäº†æŠ€æœ¯çš„å…ˆè¿›æ€§ï¼Œåˆå…·å¤‡äº†å¾ˆå¼ºçš„å®ç”¨æ€§ã€‚