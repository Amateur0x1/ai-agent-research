# OpenAI Agents SDK - é—®ç­”æ–‡æ¡£ (QA)

## ğŸ“‹ ç›®å½•
- [Sessionä¼šè¯ç®¡ç†](#sessionä¼šè¯ç®¡ç†)
- [æ•°æ®åº“è¿æ¥ä¸è¡¨åˆ›å»º](#æ•°æ®åº“è¿æ¥ä¸è¡¨åˆ›å»º)
- [Redisåˆ†å¸ƒå¼ä¼šè¯](#redisåˆ†å¸ƒå¼ä¼šè¯)
- [æ™ºèƒ½ä½“å¾ªç¯æœºåˆ¶](#æ™ºèƒ½ä½“å¾ªç¯æœºåˆ¶)
- [å¤šæ™ºèƒ½ä½“åä½œ](#å¤šæ™ºèƒ½ä½“åä½œ)
- [å·¥å…·ç³»ç»Ÿ](#å·¥å…·ç³»ç»Ÿ)
- [éƒ¨ç½²å’Œæ‰©å±•](#éƒ¨ç½²å’Œæ‰©å±•)

---

## Sessionä¼šè¯ç®¡ç†

### Q1: OpenAI Agents SDKçš„Sessionæ˜¯å¦‚ä½•åˆ›å»ºæ•°æ®åº“è¡¨å’Œè¿æ¥æ•°æ®åº“çš„ï¼Ÿ

**A:** OpenAI Agents SDKä½¿ç”¨è‡ªåŠ¨åŒ–çš„æ•°æ®åº“åˆå§‹åŒ–æœºåˆ¶ï¼š

#### SQLiteSession - è‡ªåŠ¨è¡¨åˆ›å»º
```python
from openai.agents.sessions import SQLiteSession

# è‡ªåŠ¨åˆ›å»ºæ•°æ®åº“å’Œè¡¨
session = SQLiteSession(
    user_id="user_123", 
    db_path="conversations.db"  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ä¼šè‡ªåŠ¨åˆ›å»º
)

# SDKå†…éƒ¨ä¼šè‡ªåŠ¨åˆ›å»ºå¦‚ä¸‹è¡¨ç»“æ„:
"""
CREATE TABLE conversations (
    id TEXT PRIMARY KEY,
    user_id TEXT NOT NULL,
    thread_id TEXT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    message_type TEXT,  -- 'user', 'assistant', 'tool'
    content TEXT,
    metadata JSON
);

CREATE INDEX idx_user_thread ON conversations(user_id, thread_id);
CREATE INDEX idx_timestamp ON conversations(timestamp);
"""
```

#### SQLAlchemySession - å¤šæ•°æ®åº“æ”¯æŒ
```python
from openai.agents.sessions import SQLAlchemySession

# PostgreSQLè¿æ¥
session = SQLAlchemySession.from_url(
    user_id="user_123",
    database_url="postgresql://username:password@localhost:5432/agents_db"
)

# MySQLè¿æ¥
session = SQLAlchemySession.from_url(
    user_id="user_123", 
    database_url="mysql://username:password@localhost:3306/agents_db"
)

# SDKå†…éƒ¨ä½¿ç”¨SQLAlchemy ORMå®šä¹‰è¡¨ç»“æ„:
class ConversationMessage(Base):
    __tablename__ = 'conversation_messages'
    
    id = Column(String, primary_key=True)
    user_id = Column(String, nullable=False, index=True)
    thread_id = Column(String, index=True)
    timestamp = Column(DateTime, default=datetime.utcnow)
    role = Column(String)  # 'user', 'assistant', 'tool'
    content = Column(Text)
    metadata = Column(JSON)
```

#### æ‰‹åŠ¨æ•°æ®åº“åˆå§‹åŒ–
```python
# å¦‚éœ€æ‰‹åŠ¨æ§åˆ¶æ•°æ®åº“åˆå§‹åŒ–
from openai.agents.sessions.base import create_tables

# åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¡¨
await create_tables(database_url="postgresql://...")

# æˆ–è€…ä½¿ç”¨migrations
from openai.agents.sessions.migrations import run_migrations
await run_migrations(database_url, target_version="latest")
```

---

### Q2: Redisåˆ†å¸ƒå¼ä¼šè¯æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ

**A:** Redisåˆ†å¸ƒå¼ä¼šè¯æ˜¯æŒ‡å°†ä¼šè¯æ•°æ®å­˜å‚¨åœ¨Redisé›†ç¾¤ä¸­ï¼Œæ”¯æŒå¤šä¸ªåº”ç”¨å®ä¾‹å…±äº«ä¼šè¯çŠ¶æ€ã€‚

#### Redisä¼šè¯çš„æ ¸å¿ƒæ¦‚å¿µ
```python
from openai.agents.sessions import RedisSession

# å•æœºRedis
session = RedisSession.from_url(
    user_id="user_123",
    redis_url="redis://localhost:6379/0"
)

# Redisé›†ç¾¤
session = RedisSession.from_cluster(
    user_id="user_123",
    cluster_nodes=[
        {"host": "redis-node1", "port": 6379},
        {"host": "redis-node2", "port": 6379}, 
        {"host": "redis-node3", "port": 6379}
    ]
)

# Redisæ•°æ®ç»“æ„
"""
Keyç»“æ„:
agents:session:user_123:messages -> List[Message]
agents:session:user_123:state -> Hash{key: value}
agents:session:user_123:metadata -> Hash{created_at, updated_at, etc}

æ•°æ®ç¤ºä¾‹:
LRANGE agents:session:user_123:messages 0 -1
1) {"role": "user", "content": "Hello", "timestamp": "2025-01-27T10:00:00Z"}
2) {"role": "assistant", "content": "Hi there!", "timestamp": "2025-01-27T10:00:01Z"}

HGETALL agents:session:user_123:state
1) "current_agent"
2) "customer_service"
3) "task_progress" 
4) "50%"
"""
```

#### åˆ†å¸ƒå¼ä¼šè¯çš„ä¼˜åŠ¿

**1. æ¨ªå‘æ‰©å±•èƒ½åŠ›**
```python
# å¤šä¸ªåº”ç”¨å®ä¾‹å…±äº«ä¼šè¯
# å®ä¾‹1 (æœåŠ¡å™¨A)
app1_session = RedisSession("user_123", "redis://cluster/")
await app1_session.add_message(user_message)

# å®ä¾‹2 (æœåŠ¡å™¨B) - å¯ä»¥ç«‹å³è®¿é—®ç›¸åŒä¼šè¯
app2_session = RedisSession("user_123", "redis://cluster/") 
messages = await app2_session.get_messages()  # åŒ…å«app1æ·»åŠ çš„æ¶ˆæ¯
```

**2. é«˜å¯ç”¨æ€§**
```python
# Redis Sentinelé…ç½® - è‡ªåŠ¨æ•…éšœè½¬ç§»
session = RedisSession.from_sentinel(
    user_id="user_123",
    sentinels=[
        ("sentinel1", 26379),
        ("sentinel2", 26379), 
        ("sentinel3", 26379)
    ],
    service_name="mymaster"
)
```

**3. å†…å­˜æ€§èƒ½**
```python
# TTLæ”¯æŒ - è‡ªåŠ¨è¿‡æœŸæ¸…ç†
session = RedisSession(
    user_id="user_123",
    redis_url="redis://localhost:6379",
    ttl=86400  # 24å°æ—¶åè‡ªåŠ¨è¿‡æœŸ
)

# æµå¼æ•°æ®å¤„ç†
async for message in session.stream_messages():
    await process_message(message)
```

---

## æ™ºèƒ½ä½“å¾ªç¯æœºåˆ¶

### Q3: æ™ºèƒ½ä½“å¾ªç¯å¦‚ä½•å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ï¼Ÿ

**A:** æ™ºèƒ½ä½“å¾ªç¯é€šè¿‡çŠ¶æ€æœºæ¨¡å¼å¤„ç†å¤æ‚ä»»åŠ¡ï¼š

```python
# å¤æ‚ä»»åŠ¡ç¤ºä¾‹ï¼šç”µå•†è®¢å•å¤„ç†
async def process_order_workflow(order_request, session):
    workflow_state = {
        "step": "validation",
        "order_data": order_request,
        "validation_result": None,
        "payment_result": None,
        "inventory_result": None
    }
    
    # æ™ºèƒ½ä½“å¾ªç¯ä¼šè‡ªåŠ¨ç®¡ç†çŠ¶æ€è½¬æ¢
    while not workflow_state.get("completed"):
        current_step = workflow_state["step"]
        
        if current_step == "validation":
            # éªŒè¯è®¢å•ä¿¡æ¯
            validation_agent = Agent(
                name="order_validator",
                instructions=f"éªŒè¯è®¢å•ä¿¡æ¯: {workflow_state['order_data']}",
                tools=[validate_customer_tool, validate_product_tool]
            )
            
            result = await Runner.run(validation_agent, session=session)
            workflow_state["validation_result"] = result
            workflow_state["step"] = "payment" if result.valid else "error"
            
        elif current_step == "payment":
            # å¤„ç†æ”¯ä»˜
            payment_agent = Agent(
                name="payment_processor", 
                instructions="å¤„ç†æ”¯ä»˜æµç¨‹",
                tools=[payment_gateway_tool, fraud_detection_tool]
            )
            
            result = await Runner.run(payment_agent, session=session)
            workflow_state["payment_result"] = result
            workflow_state["step"] = "inventory" if result.success else "payment_failed"
            
        elif current_step == "inventory":
            # åº“å­˜ç®¡ç†
            inventory_agent = Agent(
                name="inventory_manager",
                instructions="æ£€æŸ¥å’Œé¢„ç•™åº“å­˜", 
                tools=[inventory_check_tool, reserve_stock_tool]
            )
            
            result = await Runner.run(inventory_agent, session=session)
            workflow_state["inventory_result"] = result
            workflow_state["step"] = "completed" if result.available else "out_of_stock"
    
    return workflow_state
```

---

### Q4: å¦‚ä½•åœ¨æ™ºèƒ½ä½“é—´ä¼ é€’å¤æ‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Ÿ

**A:** OpenAI Agents SDKæä¾›å¤šç§ä¸Šä¸‹æ–‡ä¼ é€’æœºåˆ¶ï¼š

```python
# 1. Sessionçº§ä¸Šä¸‹æ–‡å…±äº«
class ContextualSession(SQLiteSession):
    def __init__(self, user_id, db_path):
        super().__init__(user_id, db_path)
        self.shared_context = {}
    
    async def set_context(self, key, value):
        self.shared_context[key] = value
        # æŒä¹…åŒ–åˆ°æ•°æ®åº“
        await self.store_metadata("context", self.shared_context)
    
    async def get_context(self, key):
        if not self.shared_context:
            # ä»æ•°æ®åº“æ¢å¤
            self.shared_context = await self.load_metadata("context", {})
        return self.shared_context.get(key)

# 2. æ™ºèƒ½ä½“é—´ä¸Šä¸‹æ–‡ä¼ é€’
class ContextAwareAgent(Agent):
    async def prepare_handoff(self, target_agent, context_data):
        """å‡†å¤‡äº¤æ¥æ—¶çš„ä¸Šä¸‹æ–‡æ•°æ®"""
        handoff_context = {
            "source_agent": self.name,
            "target_agent": target_agent.name,
            "shared_data": context_data,
            "timestamp": datetime.now().isoformat(),
            "conversation_summary": await self.summarize_conversation()
        }
        return handoff_context

# å®é™…ä½¿ç”¨
customer_service_agent = ContextAwareAgent(
    name="customer_service",
    instructions="""
    ä½ æ˜¯å®¢æœæ™ºèƒ½ä½“ã€‚å½“éœ€è¦æŠ€æœ¯æ”¯æŒæ—¶ï¼Œä¼ é€’ç»™technical_supportæ™ºèƒ½ä½“ã€‚
    ä¼ é€’æ—¶åŒ…å«å®¢æˆ·é—®é¢˜æ‘˜è¦å’Œå·²æ”¶é›†çš„ä¿¡æ¯ã€‚
    """,
    handoffs=[technical_support_agent]
)

technical_support_agent = ContextAwareAgent(
    name="technical_support", 
    instructions="""
    ä½ æ˜¯æŠ€æœ¯æ”¯æŒæ™ºèƒ½ä½“ã€‚æ¥æ”¶æ¥è‡ªcustomer_serviceçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œ
    åŒ…æ‹¬å®¢æˆ·é—®é¢˜å’Œå·²æ”¶é›†çš„è¯Šæ–­ä¿¡æ¯ã€‚
    """
)
```

---

## å¤šæ™ºèƒ½ä½“åä½œ

### Q5: å¦‚ä½•è®¾è®¡ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¥å¤„ç†å¤æ‚çš„ä¸šåŠ¡æµç¨‹ï¼Ÿ

**A:** è®¾è®¡æ¨¡å¼å’Œæœ€ä½³å®è·µï¼š

```python
# ç¤ºä¾‹ï¼šæ™ºèƒ½å†…å®¹åˆ›ä½œå¹³å°
class ContentCreationPlatform:
    def __init__(self):
        # ä¸“ä¸šåŒ–æ™ºèƒ½ä½“
        self.research_agent = Agent(
            name="researcher",
            instructions="Research topics and gather relevant information",
            tools=[web_search_tool, academic_search_tool, data_analysis_tool]
        )
        
        self.writer_agent = Agent(
            name="content_writer", 
            instructions="Create engaging content based on research",
            tools=[writing_assistant_tool, tone_analyzer_tool],
            handoffs=[self.research_agent, self.editor_agent]
        )
        
        self.editor_agent = Agent(
            name="editor",
            instructions="Review and improve content quality",
            tools=[grammar_check_tool, style_guide_tool, plagiarism_check_tool],
            handoffs=[self.writer_agent, self.visual_agent]
        )
        
        self.visual_agent = Agent(
            name="visual_designer",
            instructions="Create visual elements and layouts", 
            tools=[image_generator_tool, layout_designer_tool],
            handoffs=[self.editor_agent, self.publisher_agent]
        )
        
        self.publisher_agent = Agent(
            name="publisher",
            instructions="Format and publish content to various platforms",
            tools=[cms_tool, social_media_tool, seo_optimizer_tool]
        )
        
        # åè°ƒè€…æ™ºèƒ½ä½“
        self.coordinator = Agent(
            name="content_coordinator",
            instructions="""
            åè°ƒå†…å®¹åˆ›ä½œæµç¨‹ï¼š
            1. åˆ†æç”¨æˆ·éœ€æ±‚
            2. è·¯ç”±åˆ°åˆé€‚çš„ä¸“ä¸šæ™ºèƒ½ä½“
            3. ç›‘æ§è¿›åº¦å’Œè´¨é‡
            4. ç¡®ä¿æœ€ç»ˆäº¤ä»˜
            """,
            handoffs=[
                self.research_agent, self.writer_agent, 
                self.editor_agent, self.visual_agent, self.publisher_agent
            ]
        )

    async def create_content(self, content_request):
        """å®Œæ•´çš„å†…å®¹åˆ›ä½œæµç¨‹"""
        session = SQLiteSession(
            user_id=content_request.user_id,
            db_path="content_creation.db"
        )
        
        # ä»åè°ƒè€…å¼€å§‹
        result = await Runner.run(
            agent=self.coordinator,
            input=content_request.description,
            session=session
        )
        
        return result

# ä½¿ç”¨ç¤ºä¾‹
platform = ContentCreationPlatform()

content_request = ContentRequest(
    user_id="creator_123",
    description="åˆ›å»ºä¸€ç¯‡å…³äºAIæŠ€æœ¯è¶‹åŠ¿çš„åšå®¢æ–‡ç« ï¼ŒåŒ…æ‹¬é…å›¾å’Œç¤¾äº¤åª’ä½“æ¨å¹¿å†…å®¹",
    target_platforms=["blog", "linkedin", "twitter"],
    deadline="2025-02-01"
)

result = await platform.create_content(content_request)
```

---

## å·¥å…·ç³»ç»Ÿ

### Q6: å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰å·¥å…·å¹¶é›†æˆåˆ°æ™ºèƒ½ä½“ä¸­ï¼Ÿ

**A:** è‡ªå®šä¹‰å·¥å…·å¼€å‘æŒ‡å—ï¼š

```python
from typing import Dict, Any
from openai.agents.tools import function_tool

# 1. ç®€å•å‡½æ•°å·¥å…·
@function_tool
def calculate_roi(investment: float, return_amount: float, time_period: int) -> Dict[str, Any]:
    """è®¡ç®—æŠ•èµ„å›æŠ¥ç‡
    
    Args:
        investment: åˆå§‹æŠ•èµ„é‡‘é¢
        return_amount: å›æŠ¥é‡‘é¢  
        time_period: æŠ•èµ„æœŸé—´(æœˆ)
    
    Returns:
        åŒ…å«ROIè®¡ç®—ç»“æœçš„å­—å…¸
    """
    roi_percentage = ((return_amount - investment) / investment) * 100
    annual_roi = roi_percentage * (12 / time_period)
    
    return {
        "roi_percentage": round(roi_percentage, 2),
        "annual_roi": round(annual_roi, 2),
        "profit": return_amount - investment,
        "is_profitable": return_amount > investment
    }

# 2. å¤æ‚å·¥å…·ç±»
class DatabaseQueryTool:
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.connection = None
    
    @function_tool
    async def execute_query(self, sql: str, params: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢
        
        Args:
            sql: SQLæŸ¥è¯¢è¯­å¥
            params: æŸ¥è¯¢å‚æ•°
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        try:
            if not self.connection:
                self.connection = await create_connection(self.connection_string)
            
            result = await self.connection.execute(sql, params or {})
            rows = await result.fetchall()
            
            return {
                "success": True,
                "data": [dict(row) for row in rows],
                "row_count": len(rows)
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "data": []
            }
    
    @function_tool 
    async def get_table_schema(self, table_name: str) -> Dict[str, Any]:
        """è·å–è¡¨ç»“æ„ä¿¡æ¯"""
        schema_query = """
        SELECT column_name, data_type, is_nullable, column_default
        FROM information_schema.columns 
        WHERE table_name = %(table_name)s
        ORDER BY ordinal_position
        """
        
        return await self.execute_query(schema_query, {"table_name": table_name})

# 3. APIé›†æˆå·¥å…·
class WeatherAPITool:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.openweathermap.org/data/2.5"
    
    @function_tool
    async def get_current_weather(self, city: str, units: str = "metric") -> Dict[str, Any]:
        """è·å–å½“å‰å¤©æ°”ä¿¡æ¯"""
        url = f"{self.base_url}/weather"
        params = {
            "q": city,
            "appid": self.api_key,
            "units": units
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    return {
                        "city": data["name"],
                        "temperature": data["main"]["temp"],
                        "description": data["weather"][0]["description"],
                        "humidity": data["main"]["humidity"],
                        "wind_speed": data["wind"]["speed"]
                    }
                else:
                    return {"error": f"APIé”™è¯¯: {response.status}"}

# 4. å·¥å…·ç»„åˆä½¿ç”¨
financial_analyst_agent = Agent(
    name="financial_analyst",
    instructions="åˆ†æè´¢åŠ¡æ•°æ®å¹¶æä¾›æŠ•èµ„å»ºè®®",
    tools=[
        calculate_roi,  # å‡½æ•°å·¥å…·
        DatabaseQueryTool("postgresql://..."),  # æ•°æ®åº“å·¥å…·
        WeatherAPITool("your_api_key")  # APIå·¥å…·
    ]
)
```

---

## éƒ¨ç½²å’Œæ‰©å±•

### Q7: å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²å’Œæ‰©å±•OpenAI Agents SDKï¼Ÿ

**A:** ç”Ÿäº§éƒ¨ç½²æœ€ä½³å®è·µï¼š

```python
# 1. é…ç½®ç®¡ç†
from pydantic import BaseSettings

class AgentConfig(BaseSettings):
    # æ•°æ®åº“é…ç½®
    database_url: str = "postgresql://user:pass@localhost/agents"
    redis_url: str = "redis://localhost:6379"
    
    # OpenAIé…ç½®
    openai_api_key: str
    openai_model: str = "gpt-4"
    
    # åº”ç”¨é…ç½®
    max_concurrent_agents: int = 100
    session_timeout: int = 3600
    log_level: str = "INFO"
    
    class Config:
        env_file = ".env"

config = AgentConfig()

# 2. è¿æ¥æ± ç®¡ç†
from sqlalchemy.ext.asyncio import create_async_engine
from redis.asyncio import ConnectionPool

class AgentRuntime:
    def __init__(self, config: AgentConfig):
        self.config = config
        
        # æ•°æ®åº“è¿æ¥æ± 
        self.db_engine = create_async_engine(
            config.database_url,
            pool_size=20,
            max_overflow=30,
            pool_pre_ping=True
        )
        
        # Redisè¿æ¥æ± 
        self.redis_pool = ConnectionPool.from_url(
            config.redis_url,
            max_connections=50
        )
        
        # æ™ºèƒ½ä½“å®ä¾‹æ± 
        self.agent_pool = AgentPool(max_size=config.max_concurrent_agents)
    
    async def get_session(self, user_id: str):
        """è·å–ä¼šè¯å®ä¾‹"""
        return SQLAlchemySession.from_engine(
            user_id=user_id,
            engine=self.db_engine
        )
    
    async def run_agent(self, agent: Agent, input_data: str, user_id: str):
        """è¿è¡Œæ™ºèƒ½ä½“"""
        session = await self.get_session(user_id)
        
        try:
            async with self.agent_pool.acquire() as agent_instance:
                result = await Runner.run(
                    agent=agent_instance,
                    input=input_data,
                    session=session
                )
                return result
        finally:
            await session.close()

# 3. å¾®æœåŠ¡æ¶æ„
from fastapi import FastAPI, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(title="AI Agents Service")
runtime = AgentRuntime(config)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/agents/{agent_name}/chat")
async def chat_with_agent(
    agent_name: str,
    request: ChatRequest,
    background_tasks: BackgroundTasks
):
    """ä¸æ™ºèƒ½ä½“å¯¹è¯"""
    agent = get_agent_by_name(agent_name)
    
    # å¼‚æ­¥å¤„ç†é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡
    if request.async_mode:
        background_tasks.add_task(
            process_agent_request_async,
            agent, request.message, request.user_id
        )
        return {"status": "processing", "request_id": generate_request_id()}
    
    # åŒæ­¥å¤„ç†
    result = await runtime.run_agent(
        agent=agent,
        input_data=request.message,
        user_id=request.user_id
    )
    
    return {"result": result.content, "status": "completed"}

# 4. ç›‘æ§å’Œæ—¥å¿—
import structlog
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# é…ç½®åˆ†å¸ƒå¼è¿½è¸ª
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=14268,
)

span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# ç»“æ„åŒ–æ—¥å¿—
logger = structlog.get_logger()

@app.middleware("http")
async def log_requests(request, call_next):
    with tracer.start_as_current_span("http_request") as span:
        span.set_attribute("http.method", request.method)
        span.set_attribute("http.url", str(request.url))
        
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
        
        await logger.ainfo(
            "request_processed",
            method=request.method,
            url=str(request.url),
            status_code=response.status_code,
            process_time=process_time
        )
        
        return response

# 5. Dockeréƒ¨ç½²
"""
Dockerfile:

FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

docker-compose.yml:

version: '3.8'
services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/agents
      - REDIS_URL=redis://redis:6379
    depends_on:
      - db
      - redis
  
  db:
    image: postgres:14
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
      POSTGRES_DB: agents
    volumes:
      - postgres_data:/var/lib/postgresql/data
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

volumes:
  postgres_data:
"""
```

---

### Q8: å¦‚ä½•å¤„ç†å¤§è§„æ¨¡å¹¶å‘å’Œè´Ÿè½½å‡è¡¡ï¼Ÿ

**A:** æ‰©å±•ç­–ç•¥å’Œæ€§èƒ½ä¼˜åŒ–ï¼š

```python
# 1. æ™ºèƒ½ä½“æ± ç®¡ç†
import asyncio
from contextlib import asynccontextmanager

class AgentPool:
    def __init__(self, max_size: int = 100):
        self.max_size = max_size
        self.active_agents = {}
        self.semaphore = asyncio.Semaphore(max_size)
    
    @asynccontextmanager
    async def acquire(self, agent_name: str):
        """è·å–æ™ºèƒ½ä½“å®ä¾‹"""
        async with self.semaphore:
            if agent_name not in self.active_agents:
                self.active_agents[agent_name] = create_agent_instance(agent_name)
            
            agent = self.active_agents[agent_name]
            try:
                yield agent
            finally:
                # æ¸…ç†æˆ–é‡ç½®æ™ºèƒ½ä½“çŠ¶æ€
                await agent.reset_state()

# 2. è´Ÿè½½å‡è¡¡å™¨
class AgentLoadBalancer:
    def __init__(self):
        self.nodes = []
        self.current_index = 0
    
    def add_node(self, node_url: str):
        self.nodes.append(node_url)
    
    def get_next_node(self) -> str:
        """è½®è¯¢è´Ÿè½½å‡è¡¡"""
        if not self.nodes:
            raise Exception("No available nodes")
        
        node = self.nodes[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.nodes)
        return node
    
    async def execute_on_node(self, agent_request: AgentRequest):
        """åœ¨æœ€ä½³èŠ‚ç‚¹ä¸Šæ‰§è¡Œæ™ºèƒ½ä½“"""
        node = self.get_next_node()
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{node}/agents/execute",
                json=agent_request.dict()
            ) as response:
                return await response.json()

# 3. ç¼“å­˜å±‚
from redis.asyncio import Redis

class AgentCache:
    def __init__(self, redis_url: str):
        self.redis = Redis.from_url(redis_url)
    
    async def cache_agent_response(
        self, 
        cache_key: str, 
        response: str, 
        ttl: int = 3600
    ):
        """ç¼“å­˜æ™ºèƒ½ä½“å“åº”"""
        await self.redis.setex(cache_key, ttl, response)
    
    async def get_cached_response(self, cache_key: str) -> str:
        """è·å–ç¼“å­˜çš„å“åº”"""
        cached = await self.redis.get(cache_key)
        return cached.decode() if cached else None
    
    def generate_cache_key(self, agent_name: str, input_hash: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"agent:{agent_name}:response:{input_hash}"

# 4. åˆ†å¸ƒå¼ä»»åŠ¡é˜Ÿåˆ—
from celery import Celery

celery_app = Celery(
    'agents',
    broker='redis://localhost:6379/1',
    backend='redis://localhost:6379/2'
)

@celery_app.task(bind=True)
def process_agent_task(self, agent_config: dict, input_data: str, user_id: str):
    """å¼‚æ­¥å¤„ç†æ™ºèƒ½ä½“ä»»åŠ¡"""
    try:
        # åˆ›å»ºæ™ºèƒ½ä½“å®ä¾‹
        agent = Agent.from_config(agent_config)
        
        # æ‰§è¡Œä»»åŠ¡
        result = asyncio.run(execute_agent_sync(agent, input_data, user_id))
        
        return {
            "status": "success",
            "result": result.content,
            "execution_time": result.execution_time
        }
    except Exception as e:
        # é‡è¯•æœºåˆ¶
        if self.request.retries < 3:
            raise self.retry(countdown=60, max_retries=3)
        
        return {
            "status": "error", 
            "error": str(e)
        }
```

---

## è¿½è¸ªå’Œç›‘æ§ç³»ç»Ÿ

### Q10: ä»€ä¹ˆæ˜¯è‡ªåŠ¨è¿½è¸ªæœºåˆ¶ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦è¿½è¸ªæ™ºèƒ½ä½“ï¼Ÿ

**A:** è‡ªåŠ¨è¿½è¸ªæœºåˆ¶æ˜¯å¯¹AIæ™ºèƒ½ä½“è¿è¡Œè¿‡ç¨‹çš„å®Œæ•´è®°å½•å’Œç›‘æ§ç³»ç»Ÿï¼Œç±»ä¼¼äºåº”ç”¨ç¨‹åºçš„æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦è¿½è¸ªï¼Ÿ

**1. è°ƒè¯•å’Œé—®é¢˜å®šä½**
```python
# æ²¡æœ‰è¿½è¸ªæ—¶çš„å›°å¢ƒ
user_complaint = "æ™ºèƒ½ä½“ç»™äº†é”™è¯¯ç­”æ¡ˆ"
# é—®é¢˜: æ— æ³•çŸ¥é“æ™ºèƒ½ä½“æ‰§è¡Œäº†ä»€ä¹ˆæ­¥éª¤ï¼Œè°ƒç”¨äº†å“ªäº›å·¥å…·ï¼Œåœ¨å“ªé‡Œå‡ºé”™

# æœ‰è¿½è¸ªæ—¶çš„ä¼˜åŠ¿
with trace(workflow_name="Customer Support", group_id="thread_123"):
    result = await Runner.run(agent, user_input, session=session)

# å¯ä»¥æŸ¥çœ‹å®Œæ•´æ‰§è¡Œè½¨è¿¹:
"""
Timeline:
10:00:01 - Agentå¯åŠ¨: customer_service_agent
10:00:02 - LLMè°ƒç”¨: åˆ†æç”¨æˆ·é—®é¢˜ "æˆ‘çš„è®¢å•åœ¨å“ªé‡Œ?"
10:00:03 - å·¥å…·è°ƒç”¨: lookup_customer_info(customer_id="12345")
10:00:04 - å·¥å…·ç»“æœ: æ‰¾åˆ°å®¢æˆ·ä¿¡æ¯
10:00:05 - å·¥å…·è°ƒç”¨: check_order_status(order_id="67890") 
10:00:06 - å·¥å…·é”™è¯¯: OrderNotFound - è®¢å•å·ä¸å­˜åœ¨
10:00:07 - LLMè°ƒç”¨: å¤„ç†é”™è¯¯æƒ…å†µ
10:00:08 - æœ€ç»ˆå›å¤: "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°è¯¥è®¢å•"
"""
```

**2. æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–**
```python
# è¿½è¸ªæ€§èƒ½æŒ‡æ ‡
performance_metrics = {
    "total_execution_time": "5.2s",
    "llm_call_count": 3,
    "llm_total_time": "3.1s", 
    "tool_call_count": 2,
    "tool_total_time": "1.8s",
    "memory_usage": "45MB",
    "tokens_used": {
        "input_tokens": 850,
        "output_tokens": 340,
        "total_cost": "$0.023"
    }
}

# å‘ç°æ€§èƒ½ç“¶é¢ˆ
if performance_metrics["llm_total_time"] > threshold:
    alert("LLMå“åº”æ—¶é—´è¿‡é•¿ï¼Œéœ€è¦ä¼˜åŒ–promptæˆ–æ¨¡å‹")
```

**3. è´¨é‡ä¿è¯å’Œå®¡è®¡**
```python
# è¿½è¸ªæ™ºèƒ½ä½“å†³ç­–è¿‡ç¨‹
decision_trace = {
    "user_intent": "æŸ¥è¯¢è®¢å•çŠ¶æ€",
    "agent_reasoning": "ç”¨æˆ·æä¾›äº†è®¢å•å·ï¼Œéœ€è¦æŸ¥è¯¢è®¢å•ä¿¡æ¯",
    "tools_selected": ["lookup_customer_info", "check_order_status"],
    "handoff_decisions": [],
    "final_confidence": 0.92,
    "quality_score": 4.5
}

# åˆè§„æ€§å®¡è®¡
audit_trail = {
    "data_accessed": ["customer_info", "order_data"],
    "permissions_checked": True,
    "sensitive_data_handled": False,
    "policy_compliance": "PASSED"
}
```

#### è¿½è¸ªçš„å…·ä½“å†…å®¹

**1. æ™ºèƒ½ä½“æ‰§è¡Œè½¨è¿¹**
```python
class AgentTraceData:
    """æ™ºèƒ½ä½“è¿½è¸ªæ•°æ®ç»“æ„"""
    
    # åŸºç¡€ä¿¡æ¯
    workflow_id: str = "Customer_Support_20250127_001"
    session_id: str = "user_123_session"
    agent_name: str = "customer_service_agent"
    start_time: datetime
    end_time: datetime
    
    # æ‰§è¡Œæ­¥éª¤
    execution_steps: List[ExecutionStep] = [
        {
            "step_id": 1,
            "type": "llm_call",
            "input": "ç”¨æˆ·è¯¢é—®: æˆ‘çš„è®¢å•åœ¨å“ªé‡Œ?",
            "output": "éœ€è¦æŸ¥è¯¢å®¢æˆ·ä¿¡æ¯å’Œè®¢å•çŠ¶æ€",
            "duration": "1.2s",
            "tokens": {"input": 45, "output": 32}
        },
        {
            "step_id": 2, 
            "type": "tool_call",
            "tool_name": "lookup_customer_info",
            "arguments": {"customer_id": "12345"},
            "result": {"name": "å¼ ä¸‰", "phone": "138****1234"},
            "duration": "0.8s"
        },
        {
            "step_id": 3,
            "type": "agent_handoff", 
            "from_agent": "customer_service",
            "to_agent": "order_specialist",
            "reason": "éœ€è¦ä¸“ä¸šè®¢å•å¤„ç†",
            "context_transferred": "å®¢æˆ·ä¿¡æ¯å’ŒæŸ¥è¯¢æ„å›¾"
        }
    ]
    
    # èµ„æºä½¿ç”¨
    resource_usage: dict = {
        "total_cost": "$0.045",
        "api_calls": 5,
        "memory_peak": "67MB",
        "cpu_time": "2.1s"
    }
    
    # é”™è¯¯å’Œå¼‚å¸¸
    errors: List[dict] = [
        {
            "step_id": 2,
            "error_type": "ToolExecutionError", 
            "message": "æ•°æ®åº“è¿æ¥è¶…æ—¶",
            "recovered": True,
            "recovery_action": "é‡è¯•è¿æ¥"
        }
    ]
```

**2. LLMè°ƒç”¨è¯¦ç»†è®°å½•**
```python
class LLMCallTrace:
    """LLMè°ƒç”¨è¿½è¸ª"""
    
    call_id: str = "llm_call_001" 
    model: str = "gpt-4"
    timestamp: datetime
    
    # è¾“å…¥æ•°æ®
    input_data: dict = {
        "system_prompt": "ä½ æ˜¯ä¸€ä¸ªå®¢æœåŠ©æ‰‹...",
        "user_message": "æˆ‘çš„è®¢å•åœ¨å“ªé‡Œ?",
        "conversation_history": [...],
        "available_tools": ["lookup_customer", "check_order"]
    }
    
    # è¾“å‡ºæ•°æ®
    output_data: dict = {
        "response_text": "æˆ‘æ¥å¸®æ‚¨æŸ¥è¯¢è®¢å•ä¿¡æ¯...",
        "tool_calls": [
            {
                "function": "lookup_customer_info",
                "arguments": {"customer_id": "12345"}
            }
        ],
        "reasoning": "ç”¨æˆ·è¯¢é—®è®¢å•ï¼Œéœ€è¦å…ˆç¡®è®¤å®¢æˆ·èº«ä»½"
    }
    
    # æ€§èƒ½æŒ‡æ ‡
    performance: dict = {
        "latency": "1.5s",
        "tokens_input": 234,
        "tokens_output": 56,
        "cost": "$0.012",
        "model_confidence": 0.94
    }
```

#### è¿½è¸ªå®ç°æ–¹å¼

**1. è‡ªåŠ¨è¿½è¸ªè£…é¥°å™¨**
```python
# SDKå†…ç½®è‡ªåŠ¨è¿½è¸ª
@auto_trace
async def run_agent_with_tracing(agent: Agent, input_data: str):
    """è‡ªåŠ¨è¿½è¸ªæ™ºèƒ½ä½“è¿è¡Œ"""
    
    # å¼€å§‹è¿½è¸ª
    trace_context = TraceContext.start(
        workflow_name=f"agent_{agent.name}",
        user_id=get_current_user_id()
    )
    
    try:
        # è®°å½•å¼€å§‹çŠ¶æ€
        trace_context.log_event("agent_start", {
            "agent_name": agent.name,
            "input_preview": input_data[:100],
            "timestamp": datetime.now()
        })
        
        # æ‰§è¡Œæ™ºèƒ½ä½“
        result = await agent.run(input_data)
        
        # è®°å½•æˆåŠŸç»“æœ
        trace_context.log_event("agent_success", {
            "output_preview": str(result)[:100],
            "execution_time": trace_context.get_duration()
        })
        
        return result
        
    except Exception as e:
        # è®°å½•é”™è¯¯
        trace_context.log_event("agent_error", {
            "error_type": type(e).__name__,
            "error_message": str(e),
            "stack_trace": traceback.format_exc()
        })
        raise
    
    finally:
        # ç»“æŸè¿½è¸ªå¹¶ä¿å­˜
        await trace_context.finish_and_save()
```

**2. å·¥å…·è°ƒç”¨è¿½è¸ª**
```python
class TrackedTool:
    """å¸¦è¿½è¸ªçš„å·¥å…·åŒ…è£…å™¨"""
    
    def __init__(self, original_tool, tracer):
        self.original_tool = original_tool
        self.tracer = tracer
    
    async def execute(self, **kwargs):
        tool_trace_id = f"tool_{uuid.uuid4()}"
        
        # è®°å½•å·¥å…·è°ƒç”¨å¼€å§‹
        self.tracer.log_tool_start(tool_trace_id, {
            "tool_name": self.original_tool.name,
            "arguments": kwargs,
            "timestamp": datetime.now()
        })
        
        try:
            # æ‰§è¡ŒåŸå§‹å·¥å…·
            start_time = time.time()
            result = await self.original_tool.execute(**kwargs)
            execution_time = time.time() - start_time
            
            # è®°å½•æˆåŠŸç»“æœ
            self.tracer.log_tool_success(tool_trace_id, {
                "result": result,
                "execution_time": execution_time,
                "success": True
            })
            
            return result
            
        except Exception as e:
            # è®°å½•å·¥å…·é”™è¯¯
            self.tracer.log_tool_error(tool_trace_id, {
                "error": str(e),
                "error_type": type(e).__name__,
                "success": False
            })
            raise
```

#### å¤–éƒ¨ç›‘æ§å¹³å°é›†æˆ

**1. Logfireé›†æˆ** (Pydanticå®˜æ–¹è¿½è¸ªå¹³å°)
```python
import logfire

# é…ç½®Logfire
logfire.configure(
    token="your_logfire_token",
    service_name="ai_agents_service"
)

# è‡ªåŠ¨è¿½è¸ªæ™ºèƒ½ä½“
@logfire.instrument("agent_execution")
async def run_agent_with_logfire(agent: Agent, input_data: str):
    with logfire.span("agent_run", agent_name=agent.name) as span:
        # è®°å½•è¾“å…¥
        span.set_attribute("input_length", len(input_data))
        span.set_attribute("agent_type", type(agent).__name__)
        
        result = await agent.run(input_data)
        
        # è®°å½•è¾“å‡º
        span.set_attribute("output_length", len(str(result)))
        span.set_attribute("success", True)
        
        return result
```

**2. AgentOpsé›†æˆ** (ä¸“ä¸šAgentç›‘æ§)
```python
from agentops import track_agent

# AgentOpsé…ç½®
@track_agent(
    agent_name="customer_service",
    track_costs=True,
    track_performance=True
)
class CustomerServiceAgent(Agent):
    async def run(self, input_data):
        # AgentOpsè‡ªåŠ¨è¿½è¸ª:
        # - LLMè°ƒç”¨æ¬¡æ•°å’Œæˆæœ¬
        # - å·¥å…·ä½¿ç”¨é¢‘ç‡
        # - æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ ‡
        # - ç”¨æˆ·æ»¡æ„åº¦ç›¸å…³æ•°æ®
        return await super().run(input_data)
```

**3. è‡ªå®šä¹‰è¿½è¸ªå¤„ç†å™¨**
```python
class CustomTraceProcessor:
    """è‡ªå®šä¹‰è¿½è¸ªæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, database_url: str, alerting_service: AlertService):
        self.db = create_connection(database_url)
        self.alerting = alerting_service
    
    async def process_trace(self, trace_data: AgentTraceData):
        """å¤„ç†è¿½è¸ªæ•°æ®"""
        
        # 1. ä¿å­˜åˆ°æ•°æ®åº“
        await self.save_to_database(trace_data)
        
        # 2. å®æ—¶ç›‘æ§å‘Šè­¦
        await self.check_alerts(trace_data)
        
        # 3. æ€§èƒ½åˆ†æ
        await self.analyze_performance(trace_data)
        
        # 4. è´¨é‡è¯„ä¼°
        await self.evaluate_quality(trace_data)
    
    async def check_alerts(self, trace_data: AgentTraceData):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        
        # æ€§èƒ½å‘Šè­¦
        if trace_data.resource_usage["total_cost"] > 10.0:
            await self.alerting.send_alert(
                "HIGH_COST",
                f"æ™ºèƒ½ä½“æ‰§è¡Œæˆæœ¬è¿‡é«˜: ${trace_data.resource_usage['total_cost']}"
            )
        
        # é”™è¯¯å‘Šè­¦  
        if trace_data.errors:
            await self.alerting.send_alert(
                "EXECUTION_ERROR",
                f"æ™ºèƒ½ä½“æ‰§è¡Œå‡ºç° {len(trace_data.errors)} ä¸ªé”™è¯¯"
            )
        
        # æ€§èƒ½å‘Šè­¦
        execution_time = (trace_data.end_time - trace_data.start_time).total_seconds()
        if execution_time > 30.0:
            await self.alerting.send_alert(
                "SLOW_EXECUTION", 
                f"æ™ºèƒ½ä½“æ‰§è¡Œæ—¶é—´è¿‡é•¿: {execution_time}ç§’"
            )
```

#### è¿½è¸ªæ•°æ®åˆ†æå’Œå¯è§†åŒ–

**1. å®æ—¶ç›‘æ§ä»ªè¡¨æ¿**
```python
class AgentMonitoringDashboard:
    """æ™ºèƒ½ä½“ç›‘æ§ä»ªè¡¨æ¿"""
    
    def get_real_time_metrics(self):
        return {
            "active_agents": 23,
            "requests_per_minute": 156,
            "average_response_time": "2.3s", 
            "success_rate": "98.5%",
            "total_cost_today": "$45.67",
            "top_errors": [
                {"error": "DatabaseTimeout", "count": 5},
                {"error": "ToolNotFound", "count": 2}
            ]
        }
    
    def get_agent_performance_trends(self, agent_name: str, time_range: str):
        """è·å–æ™ºèƒ½ä½“æ€§èƒ½è¶‹åŠ¿"""
        return {
            "response_time_trend": [1.2, 1.5, 1.8, 1.4, 1.3],  # æœ€è¿‘5å¤©
            "success_rate_trend": [99.1, 98.7, 98.9, 99.2, 98.5],
            "cost_trend": [8.5, 9.2, 10.1, 8.8, 9.4],
            "user_satisfaction": [4.2, 4.5, 4.1, 4.6, 4.3]
        }
```

**2. æ™ºèƒ½åŒ–å¼‚å¸¸æ£€æµ‹**
```python
class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ"""
    
    def detect_anomalies(self, trace_data: List[AgentTraceData]):
        """æ£€æµ‹å¼‚å¸¸æ¨¡å¼"""
        
        anomalies = []
        
        # æ£€æµ‹æ€§èƒ½å¼‚å¸¸
        response_times = [t.get_duration() for t in trace_data]
        if self.is_outlier(response_times):
            anomalies.append({
                "type": "performance_anomaly",
                "description": "å“åº”æ—¶é—´å¼‚å¸¸åé«˜",
                "severity": "medium"
            })
        
        # æ£€æµ‹é”™è¯¯ç‡å¼‚å¸¸
        error_rate = len([t for t in trace_data if t.errors]) / len(trace_data)
        if error_rate > 0.05:  # è¶…è¿‡5%é”™è¯¯ç‡
            anomalies.append({
                "type": "error_rate_spike", 
                "description": f"é”™è¯¯ç‡è¾¾åˆ° {error_rate:.1%}",
                "severity": "high"
            })
        
        # æ£€æµ‹æˆæœ¬å¼‚å¸¸
        total_cost = sum(t.resource_usage.get("total_cost", 0) for t in trace_data)
        if total_cost > self.get_cost_threshold():
            anomalies.append({
                "type": "cost_anomaly",
                "description": f"æˆæœ¬å¼‚å¸¸: ${total_cost}",
                "severity": "high"
            })
        
        return anomalies
```

#### è¿½è¸ªç³»ç»Ÿçš„ä»·å€¼

**1. è¿ç»´ä»·å€¼**
- **å¿«é€Ÿé—®é¢˜å®šä½**: ç²¾ç¡®æ‰¾åˆ°å¤±è´¥çš„æ­¥éª¤å’ŒåŸå› 
- **æ€§èƒ½ä¼˜åŒ–**: è¯†åˆ«ç“¶é¢ˆï¼Œä¼˜åŒ–å“åº”æ—¶é—´
- **æˆæœ¬æ§åˆ¶**: ç›‘æ§APIè°ƒç”¨æˆæœ¬ï¼Œä¼˜åŒ–èµ„æºä½¿ç”¨

**2. ä¸šåŠ¡ä»·å€¼**
- **è´¨é‡ä¿è¯**: ç›‘æ§æ™ºèƒ½ä½“å›ç­”è´¨é‡å’Œç”¨æˆ·æ»¡æ„åº¦
- **åˆè§„å®¡è®¡**: è®°å½•æ•°æ®è®¿é—®å’Œå†³ç­–è¿‡ç¨‹
- **äº§å“æ”¹è¿›**: åŸºäºä½¿ç”¨æ•°æ®ä¼˜åŒ–æ™ºèƒ½ä½“è®¾è®¡

**3. å¼€å‘ä»·å€¼**
- **è°ƒè¯•æ”¯æŒ**: è¯¦ç»†çš„æ‰§è¡Œè½¨è¿¹å¸®åŠ©è°ƒè¯•
- **A/Bæµ‹è¯•**: æ¯”è¾ƒä¸åŒç‰ˆæœ¬æ™ºèƒ½ä½“çš„æ€§èƒ½
- **æ™ºèƒ½ä½“è¿›åŒ–**: åŸºäºè¿½è¸ªæ•°æ®æŒç»­æ”¹è¿›æ™ºèƒ½ä½“

---

## å¸¸è§é—®é¢˜æ•…éšœæ’é™¤

### Q9: å¸¸è§çš„æ€§èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼Ÿ

**A:** æ€§èƒ½ä¼˜åŒ–æŒ‡å—ï¼š

```python
# 1. è¿æ¥æ± é…ç½®ä¼˜åŒ–
optimal_db_config = {
    "pool_size": 20,           # åŸºç¡€è¿æ¥æ•°
    "max_overflow": 30,        # æœ€å¤§æº¢å‡ºè¿æ¥
    "pool_timeout": 30,        # è·å–è¿æ¥è¶…æ—¶
    "pool_recycle": 3600,      # è¿æ¥å›æ”¶æ—¶é—´
    "pool_pre_ping": True      # è¿æ¥å¥åº·æ£€æŸ¥
}

# 2. Sessionä¼˜åŒ–
class OptimizedSession(SQLiteSession):
    async def batch_add_messages(self, messages: List[Message]):
        """æ‰¹é‡æ·»åŠ æ¶ˆæ¯ï¼Œå‡å°‘æ•°æ®åº“æ“ä½œ"""
        async with self.db.begin() as transaction:
            for message in messages:
                await transaction.execute(
                    insert(self.messages_table).values(message.dict())
                )

# 3. æ™ºèƒ½ä½“å“åº”ç¼“å­˜
@lru_cache(maxsize=1000)
def get_agent_response_cached(agent_id: str, input_hash: str):
    """LRUç¼“å­˜æ™ºèƒ½ä½“å“åº”"""
    return agent_cache.get(f"{agent_id}:{input_hash}")

# 4. å¹¶å‘æ§åˆ¶
semaphore = asyncio.Semaphore(50)  # é™åˆ¶å¹¶å‘æ•°

async def controlled_agent_execution(agent, input_data):
    async with semaphore:
        return await agent.run(input_data)
```

---

*æœ€åæ›´æ–°æ—¶é—´: 2025-01-27*

## ğŸ“ æ”¯æŒè”ç³»

å¦‚æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·å‚è€ƒï¼š
- [OpenAI Agents SDK å®˜æ–¹æ–‡æ¡£](https://github.com/openai/agents-python)
- [æŠ€æœ¯åˆ†ææ–‡æ¡£](./OpenAI-Agents-SDKæŠ€æœ¯åˆ†æ.md)
- æäº¤Issueåˆ°æœ¬é¡¹ç›®çš„GitHubä»“åº“